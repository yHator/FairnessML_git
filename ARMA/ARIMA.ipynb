{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffce58f0",
   "metadata": {},
   "source": [
    "# Time series Information\n",
    "\n",
    "A TS is said to stationary if its statistical properties such as mean, variance, remain constant over time. \n",
    "\n",
    "2 reasons that make TS non-stationary:\n",
    "1. trend - varying mean over time. If we case the that on average, num of passengers growing over time.\n",
    "2. seasonality - variations at specific time-frames, people have a tendency to buy cars in a particular month because of pay increment or festivals \n",
    "\n",
    "https://machinelearningmastery.com/make-sample-forecasts-arima-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4f62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima_model import ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa870e3",
   "metadata": {},
   "source": [
    "CSV file is USW00023243.csv which is from: https://www.ncei.noaa.gov/access/search/data-search/normals-hourly-1991-2020?dataTypes=HLY-CLOD-PCTCLR&dataTypes=HLY-CLOD-PCTFEW&dataTypes=HLY-CLOD-PCTSCT&dataTypes=HLY-CLOD-PCTBKN&dataTypes=HLY-CLOD-PCTOVC&dataTypes=HLY-CLDH-NORMAL&dataTypes=HLY-HTDH-NORMAL&dataTypes=HLY-DEWP-NORMAL&dataTypes=HLY-DEWP-10PCTL&dataTypes=HLY-DEWP-90PCTL&dataTypes=HLY-HIDX-NORMAL&bbox=37.975,-122.618,37.579,-122.222&startDate=2021-10-02T00:00:00&endDate=2021-10-02T00:00:59\n",
    "\n",
    "January 1st, 24 hours\n",
    "\n",
    "Information about flags page 2: https://www.ncei.noaa.gov/data/normals-hourly/1991-2020/doc/Normals_HLY_Documentation_1991-2020.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d37014ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------NEW COLUMNS-----------------------------\n",
      "                date  HLY_TEMP_NORMAL\n",
      "0     01-01T00:00:00             48.8\n",
      "1     01-01T01:00:00             48.5\n",
      "2     01-01T02:00:00             47.6\n",
      "3     01-01T03:00:00             47.2\n",
      "4     01-01T04:00:00             46.8\n",
      "...              ...              ...\n",
      "8755  12-31T19:00:00             51.5\n",
      "8756  12-31T20:00:00             51.0\n",
      "8757  12-31T21:00:00             50.5\n",
      "8758  12-31T22:00:00             49.8\n",
      "8759  12-31T23:00:00             49.4\n",
      "\n",
      "[8727 rows x 2 columns]\n",
      "---------------------------------DESCRIBE------------------------------\n",
      "       HLY_TEMP_NORMAL\n",
      "count      8727.000000\n",
      "mean         57.432314\n",
      "std           5.892017\n",
      "min          46.400000\n",
      "25%          52.900000\n",
      "50%          57.000000\n",
      "75%          61.100000\n",
      "max          72.300000\n"
     ]
    }
   ],
   "source": [
    "#January 1st, 24 hours \n",
    "df = pd.read_csv(\"/Users/yukahatori/A_Fairness/USW00023234.csv\")\n",
    "\n",
    "#drop columns we don't need\n",
    "df.drop('STATION', axis=1, inplace=True)\n",
    "df.drop('NAME', axis=1, inplace=True)\n",
    "df.drop('LATITUDE', axis=1, inplace=True)\n",
    "df.drop('LONGITUDE', axis=1, inplace=True)\n",
    "df.drop('ELEVATION', axis=1, inplace=True)\n",
    "df.drop('month', axis=1, inplace=True)\n",
    "df.drop('hour', axis=1, inplace=True)\n",
    "df.drop('day', axis=1, inplace=True)\n",
    "\n",
    "#checks the flags and removes any rows corresponding with 'M' which means the data is missing\n",
    "measurementFlags = list(df.filter(regex='meas'))\n",
    "for flag in measurementFlags:\n",
    "    df = df[~df[flag].str.contains('M')]\n",
    "\n",
    "completenessFlags = list(df.filter(regex='meas'))\n",
    "for flag in completenessFlags:\n",
    "    df = df[~df[flag].str.contains('-9999')]\n",
    "        \n",
    "#get rid of all measurement and completeness flags\n",
    "df = df[df.columns.drop(list(df.filter(regex='meas')))]\n",
    "df = df[df.columns.drop(list(df.filter(regex='comp')))]\n",
    "\n",
    "#rename columns\n",
    "df.columns = ['date', 'HLY_TEMP_NORMAL', 'years_HLY_TEMP_NORMAL', 'HLY_TEMP_10PCTL',\n",
    "       'years_HLY_TEMP_10PCTL', 'HLY_TEMP_90PCTL', 'years_HLY_TEMP_90PCTL',\n",
    "       'HLY_DEWP_NORMAL', 'years_HLY_DEWP_NORMAL', 'HLY_DEWP_10PCTL',\n",
    "       'years_HLY_DEWPv10PCTL', 'HLY_DEWP_90PCTL', 'years_HLY_DEWP_90PCTL',\n",
    "       'HLY_PRES_NORMAL', 'years_HLY_PRES_NORMAL', 'HLY_PRES_10PCTL',\n",
    "       'years_HLY_PRES_10PCTL', 'HLY_PRES_90PCTL', 'years_HLY_PRES_90PCTL',\n",
    "       'HLY_CLDH_NORMAL', 'years_HLY_CLDH_NORMAL', 'HLY_HTDH_NORMAL',\n",
    "       'years_HLY_HTDH_NORMAL', 'HLY_CLOD_PCTCLR', 'years_HLY_CLOD_PCTCLR',\n",
    "       'HLY_CLOD_PCTFEW', 'years_HLY_CLOD_PCTFEW', 'HLY_CLOD_PCTSCT',\n",
    "       'years_HLY_CLOD_PCTSCT', 'HLY_CLOD_PCTBKN', 'years_HLY_CLOD_PCTBKN',\n",
    "       'HLY_CLOD_PCTOVC', 'years_HLY_CLOD_PCTOVC', 'HLY_HIDX_NORMAL',\n",
    "       'years_HLY_HIDX_NORMAL', 'HLY_WCHL_NORMAL', 'years_HLY_WCHL_NORMAL',\n",
    "       'HLY_WIND_AVGSPD', 'years_HLY_WIND_AVGSPD', 'HLY_WIND_PCTCLM',\n",
    "       'years_HLY_WIND_PCTCLM', 'HLY_WIND_VCTDIR', 'years_HLY_WIND_VCTDIR',\n",
    "       'HLY_WIND_VCTSPD', 'years_HLY_WIND_VCTSPD', 'HLY_WIND_1STDIR',\n",
    "       'years_HLY-WIND-1STDIR', 'HLY_WIND_1STPCT', 'years_HLY_WIND_1STPCT',\n",
    "       'HLY_WIND_2NDDIR', 'years_HLY_WIND_2NDDIR', 'HLY_WIND_2NDPCT',\n",
    "       'years_HLY_WIND_2NDPCT']\n",
    "#DROPPING COLUMNS FOR TESTING\n",
    "df = df.drop(columns=['years_HLY_TEMP_NORMAL', 'HLY_TEMP_10PCTL',\n",
    "       'years_HLY_TEMP_10PCTL', 'HLY_TEMP_90PCTL', 'years_HLY_TEMP_90PCTL',\n",
    "       'HLY_DEWP_NORMAL', 'years_HLY_DEWP_NORMAL', 'HLY_DEWP_10PCTL',\n",
    "       'years_HLY_DEWPv10PCTL', 'HLY_DEWP_90PCTL', 'years_HLY_DEWP_90PCTL',\n",
    "       'HLY_PRES_NORMAL', 'years_HLY_PRES_NORMAL', 'HLY_PRES_10PCTL',\n",
    "       'years_HLY_PRES_10PCTL', 'HLY_PRES_90PCTL', 'years_HLY_PRES_90PCTL',\n",
    "       'HLY_CLDH_NORMAL', 'years_HLY_CLDH_NORMAL', 'HLY_HTDH_NORMAL',\n",
    "       'years_HLY_HTDH_NORMAL', 'HLY_CLOD_PCTCLR', 'years_HLY_CLOD_PCTCLR',\n",
    "       'HLY_CLOD_PCTFEW', 'years_HLY_CLOD_PCTFEW', 'HLY_CLOD_PCTSCT',\n",
    "       'years_HLY_CLOD_PCTSCT', 'HLY_CLOD_PCTBKN', 'years_HLY_CLOD_PCTBKN',\n",
    "       'HLY_CLOD_PCTOVC', 'years_HLY_CLOD_PCTOVC', 'HLY_HIDX_NORMAL',\n",
    "       'years_HLY_HIDX_NORMAL', 'HLY_WCHL_NORMAL', 'years_HLY_WCHL_NORMAL',\n",
    "       'HLY_WIND_AVGSPD', 'years_HLY_WIND_AVGSPD', 'HLY_WIND_PCTCLM',\n",
    "       'years_HLY_WIND_PCTCLM', 'HLY_WIND_VCTDIR', 'years_HLY_WIND_VCTDIR',\n",
    "       'HLY_WIND_VCTSPD', 'years_HLY_WIND_VCTSPD', 'HLY_WIND_1STDIR',\n",
    "       'years_HLY-WIND-1STDIR', 'HLY_WIND_1STPCT', 'years_HLY_WIND_1STPCT',\n",
    "       'HLY_WIND_2NDDIR', 'years_HLY_WIND_2NDDIR', 'HLY_WIND_2NDPCT',\n",
    "       'years_HLY_WIND_2NDPCT'])\n",
    "print(\"-------------------------------NEW COLUMNS-----------------------------\")\n",
    "print(df)\n",
    "print('---------------------------------DESCRIBE------------------------------')\n",
    "#describe\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d6595",
   "metadata": {},
   "source": [
    "# Time series forecasting with Python using Autoregressive Moving Average (ARMA) models\n",
    "\n",
    "Used to forecast a time series. These models combine autoregressive and moving average models. In moving average models, we assume that a variable is the sum of the mean of the time series and a linear combination of noise components.\n",
    "\n",
    "The autoreggressive and moving average models can have different orders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d717e9e",
   "metadata": {},
   "source": [
    "# Split dataset \n",
    "we will hold back the last 7 days of the dataset as the test dataset and treat those time steps as out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28f4a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 8720, Validation 7\n"
     ]
    }
   ],
   "source": [
    "split_point = len(df) - 7\n",
    "dataset, validation = df[0:split_point], df[split_point:]\n",
    "\n",
    "print('Dataset %d, Validation %d' % (len(dataset), len(validation)))\n",
    "dataset.to_csv('dataset.csv', index=False)\n",
    "validation.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c3560",
   "metadata": {},
   "source": [
    "# Develop Model\n",
    "\n",
    "Make data stationary and develop a simple ARIMA model. \n",
    "\n",
    "Neutralize seasonal component? Make data stationary by taking the seasonal difference. But we can't do this because our data is an average from 1991 - 2020, so I am just going to assume it is already \"stationary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f342b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/arima_model.py:472: FutureWarning: \n",
      "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\n",
      "been deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\n",
      "between arima and model) and\n",
      "statsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n",
      "\n",
      "statsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\n",
      "is both well tested and maintained.\n",
      "\n",
      "To silence this warning and continue using ARMA and ARIMA until they are\n",
      "removed, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
      "                        FutureWarning)\n",
      "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
      "                        FutureWarning)\n",
      "\n",
      "  warnings.warn(ARIMA_DEPRECATION_WARN, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '01-01T00:00:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yv/7r0htvks26q7nhdm7l86zm7m0000gn/T/ipykernel_23212/1746522446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print summary of fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/arima_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, order, exog, dates, freq, missing)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;31m# GH 2575\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0marray_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'endog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0m_check_estimable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tools/validation/validation.py\u001b[0m in \u001b[0;36marray_like\u001b[0;34m(obj, name, dtype, ndim, maxdim, shape, order, contiguous, optional)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaxdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '01-01T00:00:00'"
     ]
    }
   ],
   "source": [
    "dataset = np.array(dataset)\n",
    "model = ARMA(dataset, order=(1,1))\n",
    "model_fit = model.fit()\n",
    "# print summary of fit model\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888abe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af7007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
